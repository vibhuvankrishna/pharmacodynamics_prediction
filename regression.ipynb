{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5861d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "!c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91896a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pytdc as it is not installed.\n",
      "WARNING: Skipping cellxgene-census as it is not installed.\n",
      "WARNING: Skipping tiledbsoma as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y pytdc cellxgene-census tiledbsoma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4581c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTDC loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89.9k/89.9k [00:00<00:00, 140kiB/s] \n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Drug_ID                                               Drug      Y\n",
      "0   Cannabinol           CCCCCc1cc(O)c2c(c1)OC(C)(C)c1ccc(C)cc1-2  16.98\n",
      "1  Carocainide            CNC(=O)Nc1c(OCCN2CCCC2)c(OC)c2occc2c1OC   0.80\n",
      "2    CGP15720A          O=C(NCCN1CCN=C1c1ccncc1)Nc1ccc(C(=O)O)cc1   0.35\n",
      "3   Astromicin  CO[C@H]1[C@@H](O)[C@H](N)[C@@H](O[C@H]2O[C@H](...   0.23\n",
      "4    Cefuzonam  CO/N=C(\\C(=O)N[C@H]1C(=O)N2C(C(=O)O)=C(CSc3cnn...   0.20\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# FIX PyTDC LOCAL IMPORT (NO INSTALL NEEDED)\n",
    "# =====================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Add current notebook directory to Python path\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "if NOTEBOOK_DIR not in sys.path:\n",
    "    sys.path.insert(0, NOTEBOOK_DIR)\n",
    "\n",
    "# 2. Verify PyTDC folder exists\n",
    "assert os.path.isdir(\"PyTDC\"), \"PyTDC folder not found in notebook directory\"\n",
    "\n",
    "# 3. Add PyTDC folder to path\n",
    "PYTDC_PATH = os.path.join(NOTEBOOK_DIR, \"PyTDC\")\n",
    "if PYTDC_PATH not in sys.path:\n",
    "    sys.path.insert(0, PYTDC_PATH)\n",
    "\n",
    "# 4. NOW import using *tdc* (correct module name)\n",
    "from tdc.single_pred import ADME, Tox\n",
    "\n",
    "print(\"PyTDC loaded successfully\")\n",
    "\n",
    "# =====================================================\n",
    "# TEST LOAD\n",
    "# =====================================================\n",
    "vdss = ADME(name=\"VDss_Lombardo\").get_data()\n",
    "print(vdss.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d708fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 82.5k/82.5k [00:00<00:00, 131kiB/s] \n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 265k/265k [00:00<00:00, 268kiB/s]  \n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 53.6k/53.6k [00:00<00:00, 162kiB/s] \n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 91.6k/91.6k [00:00<00:00, 148kiB/s] \n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 740k/740k [00:01<00:00, 464kiB/s]  \n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 45.6k/45.6k [00:00<00:00, 127kiB/s] \n",
      "Loading...\n",
      "Done!\n",
      "Downloading...\n",
      "100%|██████████| 50.2k/50.2k [00:00<00:00, 76.8kiB/s]\n",
      "Loading...\n",
      "Done!\n",
      "['tox21', 'toxcast', 'clintox', 'herg_karim', 'herg', 'herg_central', 'dili', 'skin_reaction', 'ames', 'carcinogens_lagunin', 'ld50_zhu']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('hek293', 'does not match to available values. Please double check.')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Toxicity datasets\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m herg = Tox(name=\u001b[33m\"\u001b[39m\u001b[33mhERG\u001b[39m\u001b[33m\"\u001b[39m).get_data()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m hek298 = \u001b[43mTox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEK293\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.get_data()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Optional: save all to CSV\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     26\u001b[39m caco2.to_csv(\u001b[33m\"\u001b[39m\u001b[33mcaco2.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\single_pred\\tox.py:39\u001b[39m, in \u001b[36mTox.__init__\u001b[39m\u001b[34m(self, name, path, label_name, print_stats, convert_format)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     32\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     convert_format=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     37\u001b[39m ):\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a Tox (Toxicity Prediction) dataloader object.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTox\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m print_stats:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mself\u001b[39m.print_stats()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\single_pred\\single_pred_dataset.py:70\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, name, path, label_name, print_stats, dataset_names, convert_format, raw_format)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     66\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease select a label name. You can use tdc.utils.retrieve_label_name_list(\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m             + name.lower() +\n\u001b[32m     68\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) to retrieve all available label names.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m entity1, y, entity1_idx = \u001b[43mproperty_dataset_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m.entity1 = entity1\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.y = y\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\utils\\load.py:391\u001b[39m, in \u001b[36mproperty_dataset_load\u001b[39m\u001b[34m(name, path, target, dataset_names)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    390\u001b[39m     target = \u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m name = \u001b[43mdownload_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m print_sys(\u001b[33m\"\u001b[39m\u001b[33mLoading...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    393\u001b[39m df = pd_load(name, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\utils\\load.py:60\u001b[39m, in \u001b[36mdownload_wrapper\u001b[39m\u001b[34m(name, path, dataset_names)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_wrapper\u001b[39m(name, path, dataset_names):\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"wrapper for downloading a dataset given the name and path, for csv,pkl,tsv files\u001b[39;00m\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m \u001b[33;03m        str: the exact dataset query name\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     name = \u001b[43mfuzzy_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     server_path = \u001b[33m\"\u001b[39m\u001b[33mhttps://dataverse.harvard.edu/api/access/datafile/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m name2idlist:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\utils\\misc.py:31\u001b[39m, in \u001b[36mfuzzy_search\u001b[39m\u001b[34m(name, dataset_names)\u001b[39m\n\u001b[32m     28\u001b[39m     s = name\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# print(\"========fuzzysearch=======\", dataset_names, name)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     s = \u001b[43mget_closet_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m dataset_names:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\neuro lab\\sem 8\\pharmacokinetics\\PyTDC\\tdc\\utils\\misc.py:72\u001b[39m, in \u001b[36mget_closet_match\u001b[39m\u001b[34m(predefined_tokens, test_token, threshold)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prob_max / \u001b[32m100\u001b[39m < threshold:\n\u001b[32m     71\u001b[39m     print_sys(predefined_tokens)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m         test_token, \u001b[33m\"\u001b[39m\u001b[33mdoes not match to available values. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease double check.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m token_max, prob_max / \u001b[32m100\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: ('hek293', 'does not match to available values. Please double check.')"
     ]
    }
   ],
   "source": [
    "import PyTDC\n",
    "from PyTDC.tdc.single_pred import ADME, Tox\n",
    "\n",
    "# -------------------------\n",
    "# ADME datasets\n",
    "# -------------------------\n",
    "caco2 = ADME(name=\"Caco2_Wang\").get_data()\n",
    "vdss = ADME(name=\"VDss_Lombardo\").get_data()\n",
    "ppb = ADME(name=\"PPBR_AZ\").get_data()\n",
    "half_life = ADME(name=\"Half_Life_Obach\").get_data()\n",
    "clearance = ADME(name=\"Clearance_Hepatocyte_AZ\").get_data()\n",
    "\n",
    "# CYP2C9 (often written as CYP2C9 / CYP2CQ in some notes)\n",
    "cyp2c9_inhibitor = ADME(name=\"CYP2C9_Veith\").get_data()\n",
    "cyp2c9_substrate = ADME(name=\"CYP2C9_Substrate_CarbonMangels\").get_data()\n",
    "\n",
    "# -------------------------\n",
    "# Toxicity datasets\n",
    "# -------------------------\n",
    "herg = Tox(name=\"hERG\").get_data()\n",
    "hek298 = Tox(name=\"HEK293\").get_data()\n",
    "# herg = Tox(name=\"hERG\").get_data()\n",
    "\n",
    "# -------------------------\n",
    "# Optional: save all to CSV\n",
    "# -------------------------\n",
    "caco2.to_csv(\"caco2.csv\", index=False)\n",
    "vdss.to_csv(\"vdss.csv\", index=False)\n",
    "ppb.to_csv(\"ppb.csv\", index=False)\n",
    "cyp2c9_inhibitor.to_csv(\"cyp2c9_inhibitor.csv\", index=False)\n",
    "cyp2c9_substrate.to_csv(\"cyp2c9_substrate.csv\", index=False)\n",
    "half_life.to_csv(\"half_life.csv\", index=False)\n",
    "clearance.to_csv(\"clearance.csv\", index=False)\n",
    "herg.to_csv(\"herg.csv\", index=False)\n",
    "hek298.to_csv(\"hek298.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aafc323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .tab files converted to .csv\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# CONVERT PyTDC .tab FILES TO .csv (OPTIONAL)\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"data\"   # PyTDC data folder\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".tab\"):\n",
    "        tab_path = os.path.join(DATA_DIR, file)\n",
    "        csv_path = os.path.join(DATA_DIR, file.replace(\".tab\", \".csv\"))\n",
    "\n",
    "        df = pd.read_csv(tab_path, sep=\"\\t\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"All .tab files converted to .csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac7ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET: caco2_wang.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 910 entries, 0 to 909\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Drug_ID  910 non-null    object \n",
      " 1   Drug     910 non-null    object \n",
      " 2   Y        910 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 21.5+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "                                                Drug_ID  \\\n",
      "0                                       (-)-epicatechin   \n",
      "1  (2E,4Z,8Z)-N-isobutyldodeca-2,4,10-triene-8 -ynamide   \n",
      "\n",
      "                                      Drug     Y  \n",
      "0  Oc1cc(O)c2c(c1)OC(c1ccc(O)c(O)c1)C(O)C2 -6.22  \n",
      "1         C/C=C\\C#CCC/C=C\\C=C\\C(=O)NCC(C)C -3.86  \n",
      "\n",
      "================================================================================\n",
      "DATASET: clearance_hepatocyte_az.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1213 entries, 0 to 1212\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      1213 non-null   object \n",
      " 1   X       1213 non-null   object \n",
      " 2   Y       1213 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 28.6+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "              ID                                                     X    Y\n",
      "0  CHEMBL2177601   Cc1c(C(=O)NC2C3CC4CC(C3)CC2C4)cnn1-c1ccc(C(=O)O)cc1  3.0\n",
      "1  CHEMBL1956129  C[C@@](CCc1ccc(-c2ccc(Cl)cc2)cc1)(C(=O)NO)S(C)(=O)=O  3.0\n",
      "\n",
      "================================================================================\n",
      "DATASET: cyp2c9_substrate_carbonmangels.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 669 entries, 0 to 668\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Drug_ID  669 non-null    object\n",
      " 1   Drug     669 non-null    object\n",
      " 2   Y        669 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 15.8+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "     Drug_ID                                                Drug  Y\n",
      "0   abacavir       Nc1nc(NC2CC2)c2ncn([C@H]3C=C[C@@H](CO)C3)c2n1  0\n",
      "1  abecarnil  COCc1c(C(=O)OC(C)C)ncc2[nH]c3ccc(OCc4ccccc4)cc3c12  0\n",
      "\n",
      "================================================================================\n",
      "DATASET: cyp2c9_veith.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12092 entries, 0 to 12091\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Drug_ID  12092 non-null  float64\n",
      " 1   Drug     12092 non-null  object \n",
      " 2   Y        12092 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 283.5+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "     Drug_ID                                          Drug  Y\n",
      "0  1960010.0  CCN1C(=O)/C(=C2\\SC(=S)N(CCCOC)C2=O)c2ccccc21  1\n",
      "1   644851.0              Clc1ccccc1-c1nc(-c2ccccc2)n[nH]1  1\n",
      "\n",
      "================================================================================\n",
      "DATASET: half_life_obach.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 667 entries, 0 to 666\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      667 non-null    object \n",
      " 1   X       667 non-null    object \n",
      " 2   Y       667 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 15.8+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "           ID  \\\n",
      "0  CHEMBL1754   \n",
      "1  CHEMBL1484   \n",
      "\n",
      "                                                                      X    Y  \n",
      "0                           CCN1CC(CCN2CCOCC2)C(c2ccccc2)(c2ccccc2)C1=O  4.1  \n",
      "1  COC(=O)C1=C(C)NC(C)=C(C(=O)OCCN(C)Cc2ccccc2)C1c1cccc([N+](=O)[O-])c1  4.1  \n",
      "\n",
      "================================================================================\n",
      "DATASET: herg.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 655 entries, 0 to 654\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Drug_ID  633 non-null    object \n",
      " 1   Drug     655 non-null    object \n",
      " 2   Y        655 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 15.5+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "              Drug_ID  \\\n",
      "0  DEMETHYLASTEMIZOLE   \n",
      "1           GBR-12909   \n",
      "\n",
      "                                                        Drug    Y  \n",
      "0          Oc1ccc(CCN2CCC(Nc3nc4ccccc4n3Cc3ccc(F)cc3)CC2)cc1  1.0  \n",
      "1  Fc1ccc(C(OCC[NH+]2CC[NH+](CCCc3ccccc3)CC2)c2ccc(F)cc2)cc1  1.0  \n",
      "\n",
      "================================================================================\n",
      "DATASET: ppbr_az.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2828 entries, 0 to 2827\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Drug_ID  2828 non-null   object \n",
      " 1   Drug     2828 non-null   object \n",
      " 2   Y        2828 non-null   float64\n",
      " 3   Species  2828 non-null   object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 88.5+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "         Drug_ID  \\\n",
      "0     CHEMBL1017   \n",
      "1  CHEMBL2337981   \n",
      "\n",
      "                                                             Drug      Y  \\\n",
      "0  CCCc1nc2c(C)cc(-c3nc4ccccc4n3C)cc2n1Cc1ccc(-c2ccccc2C(=O)O)cc1  98.25   \n",
      "1          CC(C)(C)NC(=O)NCCN1CCC(CO)(CNC(=O)c2cc(Cl)cc(Cl)c2)CC1  82.05   \n",
      "\n",
      "                  Species  \n",
      "0  Canis lupus familiaris  \n",
      "1  Canis lupus familiaris  \n",
      "\n",
      "================================================================================\n",
      "DATASET: vdss_lombardo.csv\n",
      "================================================================================\n",
      "\n",
      "--- df.info() ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1130 entries, 0 to 1129\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      1130 non-null   object \n",
      " 1   X       1130 non-null   object \n",
      " 2   Y       1130 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 26.6+ KB\n",
      "\n",
      "--- df.describe() ---\n",
      "\n",
      " heads\n",
      "            ID                                         X      Y\n",
      "0   Cannabinol  CCCCCc1cc(O)c2c(c1)OC(C)(C)c1ccc(C)cc1-2  16.98\n",
      "1  Carocainide   CNC(=O)Nc1c(OCCN2CCCC2)c(OC)c2occc2c1OC   0.80\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# RUN df.info() AND df.describe() ON ALL CSV FILES\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# df.head()   # or df\n",
    "\n",
    "DATA_DIR = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\data\"\n",
    "\n",
    "# list all csv files\n",
    "csv_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "\n",
    "for file in sorted(csv_files):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DATASET: {file}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    path = os.path.join(DATA_DIR, file)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    print(\"\\n--- df.info() ---\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\n--- df.describe() ---\")\n",
    "    # print(df.describe(include=\"all\"))\n",
    "    \n",
    "    print(\"\\n heads\")\n",
    "    print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc1a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_smiles_column(df):\n",
    "    if 'Drug' in df.columns:\n",
    "        df['smiles'] = df['Drug']\n",
    "    elif 'X' in df.columns:\n",
    "        df['smiles'] = df['X']\n",
    "    else:\n",
    "        raise ValueError(\"No SMILES column found\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca00d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Processed: caco2_wang.csv, shape: (910, 3)\n",
      "[DONE] Processed: clearance_hepatocyte_az.csv, shape: (1213, 3)\n",
      "[DONE] Processed: cyp2c9_substrate_carbonmangels.csv, shape: (669, 3)\n",
      "[DONE] Processed: cyp2c9_veith.csv, shape: (12092, 3)\n",
      "[DONE] Processed: half_life_obach.csv, shape: (667, 3)\n",
      "[DONE] Processed: herg.csv, shape: (655, 3)\n",
      "[DONE] Processed: ppbr_az.csv, shape: (2828, 4)\n",
      "[DONE] Processed: vdss_lombardo.csv, shape: (1130, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder containing your CSV files\n",
    "input_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\data\"\n",
    "output_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files in folder\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Function to preprocess a single dataframe\n",
    "def preprocess_df(df):\n",
    "    # Identify SMILES column\n",
    "    if 'Drug' in df.columns:\n",
    "        df['smiles'] = df['Drug']\n",
    "        df.drop(columns=['Drug'], inplace=True)\n",
    "    elif 'X' in df.columns:\n",
    "        df['smiles'] = df['X']\n",
    "        df.drop(columns=['X'], inplace=True)\n",
    "    else:\n",
    "        raise ValueError(\"No SMILES column found in df\")\n",
    "\n",
    "    # Optional: reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each CSV\n",
    "for file in csv_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Add SMILES column\n",
    "    df = preprocess_df(df)\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    out_path = os.path.join(output_folder, file)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"[DONE] Processed: {file}, shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys, AllChem\n",
    "from mordred import Calculator, descriptors\n",
    "\n",
    "# Folders\n",
    "input_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "output_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List CSV files\n",
    "csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Mordred calculator (all 2D+3D descriptors)\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"\\nProcessing {file}...\")\n",
    "    path = os.path.join(input_folder, file)\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Convert SMILES to RDKit molecules\n",
    "    df['mol'] = df['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "    \n",
    "    # Check for invalid molecules\n",
    "    invalid = df['mol'].isnull().sum()\n",
    "    if invalid > 0:\n",
    "        print(f\"  Warning: {invalid} invalid SMILES detected and will be skipped.\")\n",
    "        df = df[df['mol'].notnull()].reset_index(drop=True)\n",
    "    \n",
    "    # -------------------- Mordred Descriptors --------------------\n",
    "    print(\"  Generating Mordred descriptors...\")\n",
    "    mordred_df = calc.map(df['mol'])\n",
    "    # mordred_df = pd.concat([df.drop(columns=['mol']), mordred_df], axis=1)\n",
    "    mordred_df = pd.DataFrame(list(calc.map(df['mol'])))\n",
    "\n",
    "    mordred_path = os.path.join(output_folder, file.replace('.csv', '_mordred.csv'))\n",
    "    mordred_df.to_csv(mordred_path, index=False)\n",
    "    \n",
    "    # -------------------- MACCS Keys --------------------\n",
    "    print(\"  Generating MACCS keys...\")\n",
    "    def maccs_fp(mol):\n",
    "        fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "        arr = [int(x) for x in fp.ToBitString()]\n",
    "        return pd.Series(arr)\n",
    "    \n",
    "    maccs_df = df['mol'].apply(maccs_fp)\n",
    "    maccs_df = pd.concat([df.drop(columns=['mol']), maccs_df], axis=1)\n",
    "    maccs_path = os.path.join(output_folder, file.replace('.csv', '_maccs.csv'))\n",
    "    maccs_df.to_csv(maccs_path, index=False)\n",
    "    \n",
    "    # -------------------- RDKit Fingerprints --------------------\n",
    "    print(\"  Generating RDKit fingerprints...\")\n",
    "    def rdkit_fp(mol, n_bits=2048):\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=n_bits)\n",
    "        arr = [int(x) for x in fp.ToBitString()]\n",
    "        return pd.Series(arr)\n",
    "    \n",
    "    rdkit_df = df['mol'].apply(rdkit_fp)\n",
    "    rdkit_df = pd.concat([df.drop(columns=['mol']), rdkit_df], axis=1)\n",
    "    rdkit_path = os.path.join(output_folder, file.replace('.csv', '_rdkit.csv'))\n",
    "    rdkit_df.to_csv(rdkit_path, index=False)\n",
    "    \n",
    "    print(f\"  Done: Mordred ({mordred_df.shape}), MACCS ({maccs_df.shape}), RDKit ({rdkit_df.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd02668",
   "metadata": {},
   "source": [
    "Pipeline Overview\n",
    "\n",
    "For all 8 datasets, for all 7 combinations of descriptors, and for 3 simple deep learning models (CNN, RNN, LSTM), the pipeline will:\n",
    "\n",
    "Load the dataset and the corresponding descriptor CSVs.\n",
    "\n",
    "Merge descriptor columns according to the combination.\n",
    "\n",
    "Detect problem type:\n",
    "\n",
    "Continuous Y → Regression\n",
    "\n",
    "0/1 Y → Classification\n",
    "\n",
    "Preprocess data:\n",
    "\n",
    "Standardization for regression\n",
    "\n",
    "MinMaxScaler / no scaling for classification if needed\n",
    "\n",
    "Train/test split\n",
    "\n",
    "Train simple models:\n",
    "\n",
    "CNN: 1D Conv → Dense → Output\n",
    "\n",
    "RNN: Simple RNN → Dense → Output\n",
    "\n",
    "LSTM: LSTM → Dense → Output\n",
    "\n",
    "Permutation feature importance (top features) using Eli5 or sklearn\n",
    "\n",
    "Evaluation metrics:\n",
    "\n",
    "Regression: R2, RMSE, MAE, Spearman\n",
    "\n",
    "Classification: Accuracy, F1, ROC-AUC, PR-AUC\n",
    "\n",
    "Output:\n",
    "\n",
    "CSV with Actual vs Predicted for 10 molecules\n",
    "\n",
    "CSV with top features\n",
    "\n",
    "Metrics summary\n",
    "\n",
    "Folder structure:\n",
    "\n",
    "results/\n",
    "    caco2/\n",
    "        mordred+rdkit/\n",
    "            cnn/\n",
    "                metrics.csv\n",
    "                top_features.csv\n",
    "                actual_vs_predicted.csv\n",
    "            rnn/...\n",
    "            lstm/...\n",
    "        mordred+maccs/...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn tensorflow eli5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6d25719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: caco2_wang ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "=== Dataset: clearance_hepatocyte_az ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Failed training lstm for clearance_hepatocyte_az mordred: Input contains NaN.\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step\n",
      "Failed training lstm for clearance_hepatocyte_az rdkit: Input contains NaN.\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Failed training lstm for clearance_hepatocyte_az mordred+maccs: Input contains NaN.\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step\n",
      "Failed training rnn for clearance_hepatocyte_az mordred+rdkit+maccs: Input contains NaN.\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n",
      "Failed training lstm for clearance_hepatocyte_az mordred+rdkit+maccs: Input contains NaN.\n",
      "\n",
      "=== Dataset: cyp2c9_substrate_carbonmangels ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 174ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 170ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "=== Dataset: cyp2c9_veith ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 141ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 143ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "=== Dataset: half_life_obach ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "=== Dataset: herg ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Done: CNN, classification\n",
      "Training RNN...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Done: RNN, classification\n",
      "Training LSTM...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step\n",
      "Done: LSTM, classification\n",
      "\n",
      "=== Dataset: ppbr_az ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Failed training lstm for ppbr_az maccs: Input contains NaN.\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "=== Dataset: vdss_lombardo ===\n",
      "\n",
      "-- Descriptor combination: mordred --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Failed training lstm for vdss_lombardo mordred: Input contains NaN.\n",
      "\n",
      "-- Descriptor combination: rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "-- Descriptor combination: mordred+rdkit+maccs --\n",
      "Training CNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Done: CNN, regression\n",
      "Training RNN...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Done: RNN, regression\n",
      "Training LSTM...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step\n",
      "Done: LSTM, regression\n",
      "\n",
      "All trainings completed! Summary saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, SimpleRNN, LSTM, InputLayer\n",
    "\n",
    "# ------------------- Folders -------------------\n",
    "descriptor_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "main_cleaned_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "main_results_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\results\"\n",
    "os.makedirs(main_results_folder, exist_ok=True)\n",
    "\n",
    "# ------------------- Datasets -------------------\n",
    "datasets = [\n",
    "    \"caco2_wang\", \"clearance_hepatocyte_az\", \"cyp2c9_substrate_carbonmangels\",\n",
    "    \"cyp2c9_veith\", \"half_life_obach\", \"herg\", \"ppbr_az\", \"vdss_lombardo\"\n",
    "]\n",
    "\n",
    "descriptor_types = ['mordred', 'rdkit', 'maccs']\n",
    "\n",
    "# ------------------- Model definitions -------------------\n",
    "def build_model(model_type, input_shape, problem_type):\n",
    "    output_units = 1 if problem_type=='regression' else 1\n",
    "    output_activation = None if problem_type=='regression' else 'sigmoid'\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    if model_type=='cnn':\n",
    "        model.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "    elif model_type=='rnn':\n",
    "        model.add(SimpleRNN(32, activation='relu'))\n",
    "    elif model_type=='lstm':\n",
    "        model.add(LSTM(32, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(output_units, activation=output_activation))\n",
    "    \n",
    "    loss = 'mse' if problem_type=='regression' else 'binary_crossentropy'\n",
    "    model.compile(optimizer='adam', loss=loss)\n",
    "    return model\n",
    "\n",
    "# ------------------- Metrics -------------------\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # fixed\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    spearman = spearmanr(y_true, y_pred).correlation\n",
    "    return {'R2': r2, 'RMSE': rmse, 'MAE': mae, 'Spearman': spearman}\n",
    "\n",
    "def classification_metrics(y_true, y_pred_prob):\n",
    "    y_pred = (y_pred_prob>0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return {'Accuracy': accuracy, 'F1': f1, 'ROC-AUC': roc_auc, 'PR-AUC': pr_auc}\n",
    "\n",
    "# ------------------- Main Loop -------------------\n",
    "results_summary = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n=== Dataset: {dataset} ===\")\n",
    "    \n",
    "    # Load main cleaned dataset for Y\n",
    "    try:\n",
    "        main_csv_path = os.path.join(main_cleaned_folder, f\"{dataset}.csv\")\n",
    "        main_df = pd.read_csv(main_csv_path)\n",
    "        Y = main_df['Y'].values\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load main dataset {dataset}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Load descriptor CSVs\n",
    "    dfs = {}\n",
    "    for desc in descriptor_types:\n",
    "        try:\n",
    "            path = os.path.join(descriptor_folder, f\"{dataset}_{desc}.csv\")\n",
    "            dfs[desc] = pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {path}: {e}\")\n",
    "    \n",
    "    # Generate all combinations of descriptors (1,2,3)\n",
    "    all_combinations = []\n",
    "    for r in range(1, len(descriptor_types)+1):\n",
    "        all_combinations += list(combinations(descriptor_types, r))\n",
    "    \n",
    "    for comb in all_combinations:\n",
    "        comb_name = '+'.join(comb)\n",
    "        print(f\"\\n-- Descriptor combination: {comb_name} --\")\n",
    "        \n",
    "        # try:\n",
    "        #     # Merge descriptor columns\n",
    "        #     dfs_list = [dfs[c].drop(columns=['smiles'], errors='ignore') for c in comb]\n",
    "        #     X = pd.concat(dfs_list, axis=1)\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Failed to merge descriptors {comb_name}: {e}\")\n",
    "        #     continue\n",
    "        try:\n",
    "            # Merge descriptor columns\n",
    "            # Merge descriptor columns\n",
    "            # Merge descriptor columns\n",
    "            dfs_list = [dfs[c].drop(columns=['smiles'], errors='ignore') for c in comb]\n",
    "            X = pd.concat(dfs_list, axis=1)\n",
    "\n",
    "            # Convert all to numeric, coerce errors\n",
    "            X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Replace infinities\n",
    "            X.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "            # X.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "            # Fill NaNs\n",
    "            X.fillna(0, inplace=True)\n",
    "            \n",
    "            # Ensure no objects remain\n",
    "            X = X.astype(float)\n",
    "\n",
    "            # Verify\n",
    "            if X.isna().sum().sum() > 0:\n",
    "                print(f\"Warning: NaNs still present in {dataset} {comb_name}\")\n",
    "\n",
    "            # Reshape for CNN/RNN/LSTM\n",
    "            X_dl = X.values.reshape((X.shape[0], X.shape[1], 1))  # 3D input\n",
    "\n",
    "            Y_dl = Y[:X_dl.shape[0]]\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed merge/preprocess {dataset} {comb_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Determine problem type\n",
    "        try:\n",
    "            if set(np.unique(Y)) <= {0,1}:\n",
    "                problem_type = 'classification'\n",
    "            else:\n",
    "                problem_type = 'regression'\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to determine problem type for {dataset}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Split\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X.values, Y, test_size=0.1, random_state=42) # <<<<<<<<<<< changed test size\n",
    "            \n",
    "            # Scaling\n",
    "            scaler = StandardScaler() if problem_type=='regression' else MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Reshape for DL models\n",
    "            X_train_dl = X_train[..., np.newaxis]\n",
    "            X_test_dl = X_test[..., np.newaxis]\n",
    "        except Exception as e:\n",
    "            print(f\"Failed preprocessing for {dataset} {comb_name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        for model_type in ['cnn','rnn','lstm']:\n",
    "            try:\n",
    "                print(f\"Training {model_type.upper()}...\")\n",
    "                model = build_model(model_type, input_shape=X_train_dl.shape[1:], problem_type=problem_type)\n",
    "                model.fit(X_train_dl, y_train, epochs=1, batch_size=32, verbose=0) #<<<<<<<<<<<<<<<<< changed epoches\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test_dl).flatten()\n",
    "                \n",
    "                # Metrics\n",
    "                metrics = regression_metrics(y_test, y_pred) if problem_type=='regression' else classification_metrics(y_test, y_pred)\n",
    "                \n",
    "                # Top features (Permutation importance using surrogate)\n",
    "                surrogate = LinearRegression() if problem_type=='regression' else LogisticRegression(max_iter=1000)\n",
    "                surrogate.fit(X_train, y_train)\n",
    "                imp = permutation_importance(surrogate, X_test, y_test, n_repeats=5, random_state=42)\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': imp.importances_mean\n",
    "                }).sort_values(by='importance', ascending=False).head(10)\n",
    "                \n",
    "                # Actual vs predicted (first 10)\n",
    "                actual_vs_pred = pd.DataFrame({\n",
    "                    'Actual': y_test[:10],\n",
    "                    'Predicted': y_pred[:10]\n",
    "                })\n",
    "                \n",
    "                # Save results\n",
    "                save_folder = os.path.join(main_results_folder, dataset, comb_name, model_type)\n",
    "                os.makedirs(save_folder, exist_ok=True)\n",
    "                pd.DataFrame([metrics]).to_csv(os.path.join(save_folder,'metrics.csv'), index=False)\n",
    "                feature_importance.to_csv(os.path.join(save_folder,'top_features.csv'), index=False)\n",
    "                actual_vs_pred.to_csv(os.path.join(save_folder,'actual_vs_predicted.csv'), index=False)\n",
    "                \n",
    "                # Summary\n",
    "                results_summary.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Descriptors': comb_name,\n",
    "                    'Model': model_type,\n",
    "                    'Problem': problem_type,\n",
    "                    **metrics\n",
    "                })\n",
    "                \n",
    "                print(f\"Done: {model_type.upper()}, {problem_type}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed training {model_type} for {dataset} {comb_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "# Save overall summary\n",
    "try:\n",
    "    pd.DataFrame(results_summary).to_csv(os.path.join(main_results_folder,'summary_results.csv'), index=False)\n",
    "    print(\"\\nAll trainings completed! Summary saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save summary results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba16392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU mode\n",
      "⚠ Running on CPU\n",
      "\n",
      "================================================================================\n",
      "DATASET: ppbr_az\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_27432\\2510201199.py:199: DtypeWarning: Columns (140,141,149,150,158,159,167,168,176,177,185,186,194,195,203,204,212,213,221,222,230,231,347,348,356,357,365,366,374,375,383,384,392,393,401,402,410,411,419,420,428,429,437,438,446,447,454,455,462,463,470,471,478,479,486,487,494,495,502,503,510,511,518,519,526,527,534,535,542,543,550,551,558,559,566,567,574,575,582,583,590,591,598,599,606,607,614,615,622,623,630,631,638,639,832,848,1363,1364,1366,1582) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  desc_dfs[d] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ppbr_az | ('mordred',) | cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 251\u001b[39m\n\u001b[32m    243\u001b[39m tf.keras.backend.clear_session()\n\u001b[32m    245\u001b[39m model = ModelFactory.build(\n\u001b[32m    246\u001b[39m     model_type,\n\u001b[32m    247\u001b[39m     X_train_dl.shape[\u001b[32m1\u001b[39m:],\n\u001b[32m    248\u001b[39m     problem_type\n\u001b[32m    249\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m preds = model.predict(X_test_dl).flatten()\n\u001b[32m    262\u001b[39m metrics = (\n\u001b[32m    263\u001b[39m     regression_metrics(y_test, preds)\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m problem_type == \u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m classification_metrics(y_test, preds)\n\u001b[32m    266\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:810\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_functions_eagerly:\n\u001b[32m    809\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[38;5;28mself\u001b[39m._name, tf_function_call=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:133\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    137\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:810\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_functions_eagerly:\n\u001b[32m    809\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[38;5;28mself\u001b[39m._name, tf_function_call=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:114\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    116\u001b[39m         outputs,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    118\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     )\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:58\u001b[39m, in \u001b[36mTensorFlowTrainer.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_has_training_arg:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m         y_pred = \u001b[38;5;28mself\u001b[39m(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\sequential.py:220\u001b[39m, in \u001b[36mSequential.call\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._functional:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_functional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# Fallback: Just apply the layer sequence.\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# This typically happens if `inputs` is a nested struct.\u001b[39;00m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    227\u001b[39m         \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[32m    229\u001b[39m         \u001b[38;5;66;03m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[32m    230\u001b[39m         \u001b[38;5;66;03m# the next layer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:183\u001b[39m, in \u001b[36mFunctional.call\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m             backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\function.py:206\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    204\u001b[39m     operation = \u001b[38;5;28mself\u001b[39m._get_operation_for_node(node)\n\u001b[32m    205\u001b[39m     op = operation_fn(operation)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     outputs = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node.outputs, tree.flatten(outputs)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:644\u001b[39m, in \u001b[36moperation_fn.<locals>.call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    639\u001b[39m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[33m\"\u001b[39m\u001b[33m_call_context_args\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    640\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    641\u001b[39m     ):\n\u001b[32m    642\u001b[39m         kwargs[name] = value\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\regularization\\dropout.py:60\u001b[39m, in \u001b[36mDropout.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rate > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnoise_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnoise_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseed_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\random.py:86\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(inputs, rate, noise_shape, seed)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdropout\u001b[39m(inputs, rate, noise_shape=\u001b[38;5;28;01mNone\u001b[39;00m, seed=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     seed = _cast_seed(\u001b[43mdraw_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     87\u001b[39m     noise_shape = _get_concrete_noise_shape(inputs, noise_shape)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.nn.experimental.stateless_dropout(\n\u001b[32m     89\u001b[39m         inputs,\n\u001b[32m     90\u001b[39m         rate=rate,\n\u001b[32m     91\u001b[39m         noise_shape=noise_shape,\n\u001b[32m     92\u001b[39m         seed=seed,\n\u001b[32m     93\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\random\\seed_generator.py:152\u001b[39m, in \u001b[36mdraw_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m random_seed_dtype\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, SeedGenerator):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mseed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seed, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_tensor([seed, \u001b[32m0\u001b[39m], dtype=random_seed_dtype())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\random\\seed_generator.py:99\u001b[39m, in \u001b[36mSeedGenerator.next\u001b[39m\u001b[34m(self, ordered)\u001b[39m\n\u001b[32m     97\u001b[39m seed_state = \u001b[38;5;28mself\u001b[39m.state\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Use * 1 to create a copy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m new_seed_value = \u001b[43mseed_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ordered:\n\u001b[32m    101\u001b[39m     increment = \u001b[38;5;28mself\u001b[39m.backend.convert_to_tensor(\n\u001b[32m    102\u001b[39m         np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m]), dtype=seed_state.dtype\n\u001b[32m    103\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1016\u001b[39m, in \u001b[36mVariable._OverloadOperator.<locals>._run_op\u001b[39m\u001b[34m(a, *args, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_op\u001b[39m(a, *args, **kwargs):\n\u001b[32m   1015\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_oper(\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:658\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:843\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    841\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    846\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    847\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    848\u001b[39m   record.record_operation(\n\u001b[32m    849\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    850\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    851\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat.forward_compatible(\u001b[32m2022\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m):\n\u001b[32m    832\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m result = \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m._dtype, \u001b[38;5;28mself\u001b[39m.handle, result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:534\u001b[39m, in \u001b[36mread_variable_op\u001b[39m\u001b[34m(resource, dtype, name)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    533\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadVariableOp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    537\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FULL ADMET DL PIPELINE (GPU READY + SAFE) ig working code\n",
    "# =========================================================\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    roc_auc_score, accuracy_score, f1_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---------------- TF SETUP ----------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, Flatten,\n",
    "    SimpleRNN, LSTM,\n",
    "    InputLayer, Dropout, BatchNormalization, MaxPooling1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# ---------------- GPU SAFETY ----------------\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        print(\"GPU ready\")\n",
    "    else:\n",
    "        print(\"CPU mode\")\n",
    "except Exception as e:\n",
    "    print(\"GPU setup failed:\", e)\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"✓ GPU enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"⚠ Running on CPU\")\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "descriptor_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "cleaned_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "results_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\results\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# DATASETS\n",
    "# =========================================================\n",
    "datasets = [\n",
    "    # \"caco2_wang\",\n",
    "    # \"clearance_hepatocyte_az\",\n",
    "    # \"cyp2c9_substrate_carbonmangels\",\n",
    "    # \"cyp2c9_veith\",\n",
    "    # \"half_life_obach\",\n",
    "    # \"herg\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\"\n",
    "]\n",
    "\n",
    "descriptor_types = [\"mordred\", \"rdkit\", \"maccs\"]\n",
    "model_types = [\"cnn\", \"rnn\", \"lstm\"]\n",
    "\n",
    "# =========================================================\n",
    "# CALLBACK\n",
    "# =========================================================\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# MODEL FACTORY\n",
    "# =========================================================\n",
    "class ModelFactory:\n",
    "\n",
    "    @staticmethod\n",
    "    def build(model_type, input_shape, problem_type):\n",
    "        try:\n",
    "            model = Sequential()\n",
    "            model.add(InputLayer(input_shape=input_shape))\n",
    "\n",
    "            # -------- CNN --------\n",
    "            if model_type == \"cnn\":\n",
    "                for f in [32, 64, 64, 128, 128]:\n",
    "                    model.add(Conv1D(\n",
    "                        f, 3, padding=\"same\", activation=\"relu\",\n",
    "                        kernel_regularizer=l2(1e-4)\n",
    "                    ))\n",
    "                    model.add(BatchNormalization())\n",
    "                    model.add(MaxPooling1D(2))\n",
    "                    model.add(Dropout(0.25))\n",
    "                model.add(Flatten())\n",
    "\n",
    "            # -------- RNN --------\n",
    "            elif model_type == \"rnn\":\n",
    "                model.add(SimpleRNN(128, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(SimpleRNN(128, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(SimpleRNN(64, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(SimpleRNN(64))\n",
    "\n",
    "            # -------- LSTM --------\n",
    "            elif model_type == \"lstm\":\n",
    "                model.add(LSTM(128, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(128, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(LSTM(64))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown model type\")\n",
    "\n",
    "            # -------- HEAD --------\n",
    "            model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4)))\n",
    "            model.add(Dropout(0.4))\n",
    "\n",
    "            if problem_type == \"regression\":\n",
    "                model.add(Dense(1))\n",
    "                model.compile(\n",
    "                    optimizer=\"adam\",\n",
    "                    loss=\"mse\",\n",
    "                    metrics=[\"mae\"]\n",
    "                )\n",
    "            else:\n",
    "                model.add(Dense(1, activation=\"sigmoid\"))\n",
    "                model.compile(\n",
    "                    optimizer=\"adam\",\n",
    "                    loss=\"binary_crossentropy\",\n",
    "                    metrics=[\"accuracy\"]\n",
    "                )\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# =========================================================\n",
    "# METRICS\n",
    "# =========================================================\n",
    "def regression_metrics(y, p):\n",
    "    return {\n",
    "        \"R2\": r2_score(y, p),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y, p)),\n",
    "        \"MAE\": mean_absolute_error(y, p),\n",
    "        \"Spearman\": spearmanr(y, p).correlation\n",
    "    }\n",
    "\n",
    "def classification_metrics(y, p):\n",
    "    yb = (p > 0.5).astype(int)\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, yb),\n",
    "        \"F1\": f1_score(y, yb),\n",
    "        \"ROC_AUC\": roc_auc_score(y, p),\n",
    "        \"PR_AUC\": auc(rec, prec)\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# MAIN LOOP\n",
    "# =========================================================\n",
    "summary = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*80}\\nDATASET: {dataset}\\n{'='*80}\")\n",
    "\n",
    "    try:\n",
    "        y_df = pd.read_csv(os.path.join(cleaned_folder, f\"{dataset}.csv\"))\n",
    "        y = y_df[\"Y\"].values\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "    desc_dfs = {}\n",
    "    for d in descriptor_types:\n",
    "        try:\n",
    "            desc_dfs[d] = pd.read_csv(\n",
    "                os.path.join(descriptor_folder, f\"{dataset}_{d}.csv\")\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for r in range(1, len(descriptor_types) + 1):\n",
    "        for comb in combinations(descriptor_types, r):\n",
    "\n",
    "            try:\n",
    "                X = pd.concat(\n",
    "                    [desc_dfs[c].drop(columns=[\"smiles\"], errors=\"ignore\") for c in comb],\n",
    "                    axis=1\n",
    "                )\n",
    "                X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            problem_type = (\n",
    "                \"classification\"\n",
    "                if set(np.unique(y)) <= {0, 1}\n",
    "                else \"regression\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X.values, y, test_size=0.1, random_state=42\n",
    "                )\n",
    "\n",
    "                scaler = StandardScaler() if problem_type == \"regression\" else MinMaxScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "                X_train_dl = X_train[..., None]\n",
    "                X_test_dl = X_test[..., None]\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            for model_type in model_types:\n",
    "                print(f\"Training {dataset} | {comb} | {model_type}\")\n",
    "\n",
    "                try:\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                    model = ModelFactory.build(\n",
    "                        model_type,\n",
    "                        X_train_dl.shape[1:],\n",
    "                        problem_type\n",
    "                    )\n",
    "\n",
    "                    model.fit(\n",
    "                        X_train_dl, y_train,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=0\n",
    "                    )\n",
    "\n",
    "                    preds = model.predict(X_test_dl).flatten()\n",
    "\n",
    "                    metrics = (\n",
    "                        regression_metrics(y_test, preds)\n",
    "                        if problem_type == \"regression\"\n",
    "                        else classification_metrics(y_test, preds)\n",
    "                    )\n",
    "\n",
    "                    save_dir = os.path.join(\n",
    "                        results_folder, dataset, \"+\".join(comb), model_type\n",
    "                    )\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "                    pd.DataFrame([metrics]).to_csv(\n",
    "                        os.path.join(save_dir, \"metrics.csv\"), index=False\n",
    "                    )\n",
    "\n",
    "                    summary.append({\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Descriptors\": \"+\".join(comb),\n",
    "                        \"Model\": model_type,\n",
    "                        \"Problem\": problem_type,\n",
    "                        **metrics\n",
    "                    })\n",
    "\n",
    "                    print(\"✓ Done\")\n",
    "\n",
    "                except Exception:\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "\n",
    "# =========================================================\n",
    "# SAVE SUMMARY\n",
    "# =========================================================\n",
    "pd.DataFrame(summary).to_csv(\n",
    "    os.path.join(results_folder, \"summary_results.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nALL TRAINING COMPLETED ✅\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a981c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using device: cuda\n",
      "\n",
      "================================================================================\n",
      "DATASET: ppbr_az\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_27432\\1014152896.py:228: DtypeWarning: Columns (140,141,149,150,158,159,167,168,176,177,185,186,194,195,203,204,212,213,221,222,230,231,347,348,356,357,365,366,374,375,383,384,392,393,401,402,410,411,419,420,428,429,437,438,446,447,454,455,462,463,470,471,478,479,486,487,494,495,502,503,510,511,518,519,526,527,534,535,542,543,550,551,558,559,566,567,574,575,582,583,590,591,598,599,606,607,614,615,622,623,630,631,638,639,832,848,1363,1364,1366,1582) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  desc_dfs[d] = pd.read_csv(os.path.join(descriptor_folder, f\"{dataset}_{d}.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ppbr_az | ('mordred',) | cnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_27432\\1014152896.py\", line 275, in <module>\n",
      "    model = train_model(model, train_loader, val_loader, problem_type)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_27432\\1014152896.py\", line 181, in train_model\n",
      "    out = model(X_batch)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 310, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 306, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Given groups=1, weight of size [32, 1, 3], expected input[32, 1613, 1] to have 1 channels, but got 1613 channels instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ppbr_az | ('mordred',) | rnn\n",
      "✓ Done\n",
      "Training ppbr_az | ('mordred',) | lstm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 275\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, problem_type, epochs, lr)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     X_batch, y_batch = \u001b[43mX_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y_batch.to(device)\n\u001b[32m    180\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 306\u001b[39m\n\u001b[32m    303\u001b[39m                     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Done\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    305\u001b[39m                 \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m                     \u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_exc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m                     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# SAVE SUMMARY\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\traceback.py:181\u001b[39m, in \u001b[36mprint_exc\u001b[39m\u001b[34m(limit, file, chain)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() failed>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# --\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_exc\u001b[39m(limit=\u001b[38;5;28;01mNone\u001b[39;00m, file=\u001b[38;5;28;01mNone\u001b[39;00m, chain=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Shorthand for 'print_exception(*sys.exc_info(), limit, file)'.\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m     print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#gpu\n",
    "\n",
    "# =========================================================\n",
    "# FULL ADMET DL PIPELINE (PyTorch GPU READY + SAFE)\n",
    "# =========================================================\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    roc_auc_score, accuracy_score, f1_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---------------- PyTorch Setup ----------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"✓ Using device:\", device)\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "descriptor_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "cleaned_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "results_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\results\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# DATASETS\n",
    "# =========================================================\n",
    "datasets = [\n",
    "    # \"caco2_wang\",\n",
    "    # \"clearance_hepatocyte_az\",\n",
    "    # \"cyp2c9_substrate_carbonmangels\",\n",
    "    # \"cyp2c9_veith\",\n",
    "    # \"half_life_obach\",\n",
    "    # \"herg\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\"\n",
    "]\n",
    "\n",
    "descriptor_types = [\"mordred\", \"rdkit\", \"maccs\"]\n",
    "model_types = [\"cnn\", \"rnn\", \"lstm\"]\n",
    "\n",
    "# =========================================================\n",
    "# METRICS\n",
    "# =========================================================\n",
    "def regression_metrics(y, p):\n",
    "    return {\n",
    "        \"R2\": r2_score(y, p),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y, p)),\n",
    "        \"MAE\": mean_absolute_error(y, p),\n",
    "        \"Spearman\": spearmanr(y, p).correlation\n",
    "    }\n",
    "\n",
    "def classification_metrics(y, p):\n",
    "    yb = (p > 0.5).astype(int)\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, yb),\n",
    "        \"F1\": f1_score(y, yb),\n",
    "        \"ROC_AUC\": roc_auc_score(y, p),\n",
    "        \"PR_AUC\": auc(rec, prec)\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# DATASET CLASS\n",
    "# =========================================================\n",
    "class ADMETDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# =========================================================\n",
    "# MODEL FACTORY\n",
    "# =========================================================\n",
    "class ModelFactory:\n",
    "\n",
    "    @staticmethod\n",
    "    def build(model_type, input_dim, problem_type):\n",
    "        try:\n",
    "            if model_type == \"cnn\":\n",
    "                model = nn.Sequential(\n",
    "                    nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(32),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.Dropout(0.25),\n",
    "                    \n",
    "                    nn.Conv1d(32, 64, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(64),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.Dropout(0.25),\n",
    "                    \n",
    "                    nn.Conv1d(64, 128, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.MaxPool1d(2),\n",
    "                    nn.Dropout(0.25),\n",
    "\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear((input_dim // 8) * 128, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.4),\n",
    "                    nn.Linear(128, 1 if problem_type==\"regression\" else 1)\n",
    "                )\n",
    "            elif model_type == \"rnn\":\n",
    "                class RNNModel(nn.Module):\n",
    "                    def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.rnn = nn.RNN(input_size=1, hidden_size=128, num_layers=3, batch_first=True)\n",
    "                        self.fc1 = nn.Linear(128, 128)\n",
    "                        self.dropout = nn.Dropout(0.4)\n",
    "                        self.out = nn.Linear(128, 1)\n",
    "                    \n",
    "                    def forward(self, x):\n",
    "                        x,_ = self.rnn(x)\n",
    "                        x = x[:, -1, :]\n",
    "                        x = F.relu(self.fc1(x))\n",
    "                        x = self.dropout(x)\n",
    "                        x = self.out(x)\n",
    "                        return x\n",
    "                model = RNNModel()\n",
    "            elif model_type == \"lstm\":\n",
    "                class LSTMModel(nn.Module):\n",
    "                    def __init__(self):\n",
    "                        super().__init__()\n",
    "                        self.lstm = nn.LSTM(input_size=1, hidden_size=128, num_layers=3, batch_first=True)\n",
    "                        self.fc1 = nn.Linear(128, 128)\n",
    "                        self.dropout = nn.Dropout(0.4)\n",
    "                        self.out = nn.Linear(128, 1)\n",
    "                    \n",
    "                    def forward(self, x):\n",
    "                        x,_ = self.lstm(x)\n",
    "                        x = x[:, -1, :]\n",
    "                        x = F.relu(self.fc1(x))\n",
    "                        x = self.dropout(x)\n",
    "                        x = self.out(x)\n",
    "                        return x\n",
    "                model = LSTMModel()\n",
    "            else:\n",
    "                raise ValueError(\"Unknown model type\")\n",
    "            return model.to(device)\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "# =========================================================\n",
    "# TRAINING LOOP\n",
    "# =========================================================\n",
    "def train_model(model, train_loader, val_loader, problem_type, epochs=200, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss() if problem_type==\"regression\" else nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    patience = 20\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X_batch)\n",
    "            out = out.squeeze(-1)\n",
    "            loss = criterion(out, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                out = model(X_batch).squeeze(-1)\n",
    "                val_loss += criterion(out, y_batch).item() * len(y_batch)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# MAIN LOOP\n",
    "# =========================================================\n",
    "summary = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*80}\\nDATASET: {dataset}\\n{'='*80}\")\n",
    "\n",
    "    try:\n",
    "        y_df = pd.read_csv(os.path.join(cleaned_folder, f\"{dataset}.csv\"))\n",
    "        y = y_df[\"Y\"].values.astype(np.float32)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "    desc_dfs = {}\n",
    "    for d in descriptor_types:\n",
    "        try:\n",
    "            desc_dfs[d] = pd.read_csv(os.path.join(descriptor_folder, f\"{dataset}_{d}.csv\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for r in range(1, len(descriptor_types)+1):\n",
    "        for comb in combinations(descriptor_types, r):\n",
    "            try:\n",
    "                X = pd.concat(\n",
    "                    [desc_dfs[c].drop(columns=[\"smiles\"], errors=\"ignore\") for c in comb],\n",
    "                    axis=1\n",
    "                )\n",
    "                X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0).values.astype(np.float32)\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            problem_type = \"classification\" if set(np.unique(y)) <= {0,1} else \"regression\"\n",
    "\n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size=0.1, random_state=42\n",
    "                )\n",
    "\n",
    "                scaler = StandardScaler() if problem_type==\"regression\" else MinMaxScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "                # Add channel dimension for PyTorch\n",
    "                X_train_dl = X_train[..., None]\n",
    "                X_test_dl = X_test[..., None]\n",
    "\n",
    "                # DataLoaders\n",
    "                train_ds = ADMETDataset(X_train_dl, y_train)\n",
    "                test_ds = ADMETDataset(X_test_dl, y_test)\n",
    "                train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "                val_loader = DataLoader(test_ds, batch_size=32)\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            for model_type in model_types:\n",
    "                print(f\"Training {dataset} | {comb} | {model_type}\")\n",
    "\n",
    "                try:\n",
    "                    model = ModelFactory.build(model_type, X_train_dl.shape[1], problem_type)\n",
    "                    if model is None: continue\n",
    "\n",
    "                    model = train_model(model, train_loader, val_loader, problem_type)\n",
    "\n",
    "                    # Predict\n",
    "                    model.eval()\n",
    "                    preds = []\n",
    "                    with torch.no_grad():\n",
    "                        for X_batch, _ in val_loader:\n",
    "                            X_batch = X_batch.to(device)\n",
    "                            out = model(X_batch).squeeze(-1).cpu().numpy()\n",
    "                            if problem_type==\"classification\":\n",
    "                                out = 1 / (1 + np.exp(-out))  # sigmoid\n",
    "                            preds.append(out)\n",
    "                    preds = np.concatenate(preds)\n",
    "\n",
    "                    metrics = regression_metrics(y_test, preds) if problem_type==\"regression\" else classification_metrics(y_test, preds)\n",
    "\n",
    "                    save_dir = os.path.join(results_folder, dataset, \"+\".join(comb), model_type)\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    pd.DataFrame([metrics]).to_csv(os.path.join(save_dir, \"metrics.csv\"), index=False)\n",
    "\n",
    "                    summary.append({\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Descriptors\": \"+\".join(comb),\n",
    "                        \"Model\": model_type,\n",
    "                        \"Problem\": problem_type,\n",
    "                        **metrics\n",
    "                    })\n",
    "\n",
    "                    print(\"✓ Done\")\n",
    "\n",
    "                except Exception:\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "\n",
    "# =========================================================\n",
    "# SAVE SUMMARY\n",
    "# =========================================================\n",
    "pd.DataFrame(summary).to_csv(\n",
    "    os.path.join(results_folder, \"summary_results.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nALL TRAINING COMPLETED ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa6475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "⚠ No GPU detected → running on CPU\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# TENSORFLOW + GPU CHECK (TF2)\n",
    "# ===============================\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✓ GPU detected: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"GPU memory growth error:\", e)\n",
    "else:\n",
    "    print(\"⚠ No GPU detected → running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1490eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22b7b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.10.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for tensorflow==2.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall tensorflow\n",
    "! pip install tensorflow==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1db8bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ DATASET: caco2_wang ================\n",
      "\n",
      "--- Descriptor set: mordred ---\n",
      "❌ No target column found, skipping\n",
      "--- Descriptor set: rdkit ---\n",
      "Epoch 1/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.8239 - val_loss: 2.5589\n",
      "Epoch 2/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8390 - val_loss: 0.9112\n",
      "Epoch 3/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0264 - val_loss: 0.6867\n",
      "Epoch 4/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7141 - val_loss: 0.7131\n",
      "Epoch 5/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3367 - val_loss: 0.6162\n",
      "Epoch 6/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2510 - val_loss: 0.7778\n",
      "Epoch 7/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1704 - val_loss: 0.5685\n",
      "Epoch 8/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1403 - val_loss: 0.7240\n",
      "Epoch 9/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1569 - val_loss: 0.6256\n",
      "Epoch 10/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1440 - val_loss: 0.8207\n",
      "Epoch 11/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1827 - val_loss: 0.6243\n",
      "Epoch 12/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9845 - val_loss: 1.0149\n",
      "Epoch 13/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5110 - val_loss: 0.7282\n",
      "Epoch 14/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4865 - val_loss: 0.8470\n",
      "Epoch 15/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3146 - val_loss: 0.6415\n",
      "Epoch 16/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2259 - val_loss: 0.9094\n",
      "Epoch 17/100\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1592 - val_loss: 0.7804\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     rmse = \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     r2 = r2_score(y_test, y_pred)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | R²: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:196\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m params.apply_defaults()\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3212\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3209\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3210\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3211\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3201\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3199\u001b[39m         arguments[kwargs_param.name] = kwargs\n\u001b[32m   3200\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3201\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3202\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3203\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Folder paths (YOUR DATA)\n",
    "# =========================\n",
    "descriptor_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "main_cleaned_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors_cleaned\"\n",
    "main_results_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\results\"\n",
    "\n",
    "os.makedirs(main_cleaned_folder, exist_ok=True)\n",
    "os.makedirs(main_results_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Datasets & descriptors\n",
    "# =========================\n",
    "datasets = [\n",
    "    \"caco2_wang\",\n",
    "    \"clearance_hepatocyte_az\",\n",
    "    \"cyp2c9_substrate_carbonmangels\",\n",
    "    \"cyp2c9_veith\",\n",
    "    \"half_life_obach\",\n",
    "    \"herg\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\"\n",
    "]\n",
    "\n",
    "descriptor_types = [\"mordred\", \"rdkit\", \"maccs\"]\n",
    "\n",
    "SMILES_COL = \"SMILES\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Model builder (defined ONCE)\n",
    "# =========================\n",
    "def build_model(input_dim, problem_type):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    ])\n",
    "\n",
    "    if problem_type == \"classification\":\n",
    "        model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"mse\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main loop\n",
    "# =========================\n",
    "for dataset in datasets:\n",
    "\n",
    "    print(f\"\\n================ DATASET: {dataset} ================\\n\")\n",
    "\n",
    "    for desc in descriptor_types:\n",
    "\n",
    "        print(f\"--- Descriptor set: {desc} ---\")\n",
    "\n",
    "        file_path = os.path.join(descriptor_folder, f\"{dataset}_{desc}.csv\")\n",
    "\n",
    "        # ---------- File existence check ----------\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        # ---------- Load ----------\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        # ---------- Identify target column ----------\n",
    "        possible_targets = [\n",
    "            c for c in df.columns\n",
    "            if c.lower() in [\"y\", \"label\", \"target\", \"value\"]\n",
    "        ]\n",
    "\n",
    "        if len(possible_targets) == 0:\n",
    "            print(\"❌ No target column found, skipping\")\n",
    "            continue\n",
    "\n",
    "        TARGET_COL = possible_targets[0]\n",
    "\n",
    "        # ---------- Split X / Y ----------\n",
    "        Y = df[TARGET_COL].values\n",
    "\n",
    "        drop_cols = [TARGET_COL]\n",
    "        if SMILES_COL in df.columns:\n",
    "            drop_cols.append(SMILES_COL)\n",
    "\n",
    "        X = df.drop(columns=drop_cols)\n",
    "\n",
    "        # ---------- Fix Mordred mixed dtypes ----------\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X.fillna(0, inplace=True)\n",
    "\n",
    "        # ---------- Remove useless columns ----------\n",
    "        X = X.loc[:, X.var() > 0]\n",
    "\n",
    "        if X.shape[0] < 5 or X.shape[1] == 0:\n",
    "            print(\"❌ Not enough usable data, skipping\")\n",
    "            continue\n",
    "\n",
    "        # ---------- Detect problem type ----------\n",
    "        unique_y = np.unique(Y)\n",
    "        problem_type = (\n",
    "            \"classification\"\n",
    "            if set(unique_y).issubset({0, 1})\n",
    "            else \"regression\"\n",
    "        )\n",
    "\n",
    "        # ---------- Train-test split ----------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X.values,\n",
    "            Y,\n",
    "            test_size=0.1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # ---------- Scaling ----------\n",
    "        scaler = MinMaxScaler() if problem_type == \"classification\" else StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # ---------- Model ----------\n",
    "        model = build_model(X_train.shape[1], problem_type)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # ---------- Train ----------\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # ---------- Evaluate ----------\n",
    "        y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "        if problem_type == \"classification\":\n",
    "            y_pred_bin = (y_pred >= 0.5).astype(int)\n",
    "            acc = accuracy_score(y_test, y_pred_bin)\n",
    "            print(f\"✅ Accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"✅ RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "        print(\"✅ Training successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid complexity cpu ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6158aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ DATASET: ppbr_az ================\n",
      "\n",
      "--- Descriptor: mordred ---\n",
      "❌ Target column not found\n",
      "--- Descriptor: rdkit ---\n",
      "🚀 Training CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m43/72\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 2363.9221"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 Training CNN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m cnn = build_cnn(X_train_seq.shape[\u001b[32m1\u001b[39m:], problem_type)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[43mcnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m y_pred = cnn.predict(X_test_seq).ravel()\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m problem_type == \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CPU-ONLY ADMET DL PIPELINE\n",
    "# CNN + RNN + LSTM (MEDIUM COMPLEXITY)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- FORCE CPU ----------------\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, MaxPooling1D, Flatten,\n",
    "    SimpleRNN, LSTM, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "descriptor_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\descriptors\"\n",
    "results_folder = r\"E:\\neuro lab\\sem 8\\pharmacokinetics\\results\"\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "datasets = [\n",
    "    # \"caco2_wang\",\n",
    "    # \"clearance_hepatocyte_az\",\n",
    "    # \"cyp2c9_substrate_carbonmangels\",\n",
    "    # \"cyp2c9_veith\",\n",
    "    # \"half_life_obach\",\n",
    "    # \"herg\",\n",
    "    \"ppbr_az\",\n",
    "    \"vdss_lombardo\"\n",
    "]\n",
    "\n",
    "descriptor_types = [\"mordred\", \"rdkit\", \"maccs\"]\n",
    "\n",
    "SMILES_COL = \"SMILES\"\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 120\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MODEL BUILDERS\n",
    "# =========================================================\n",
    "def build_cnn(input_shape, problem_type):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, 3, activation=\"relu\", input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "\n",
    "        Conv1D(128, 3, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3)\n",
    "    ])\n",
    "\n",
    "    if problem_type == \"classification\":\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_rnn(input_shape, problem_type):\n",
    "    model = Sequential([\n",
    "        SimpleRNN(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        SimpleRNN(64),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation=\"relu\")\n",
    "    ])\n",
    "\n",
    "    if problem_type == \"classification\":\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_lstm(input_shape, problem_type):\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation=\"relu\")\n",
    "    ])\n",
    "\n",
    "    if problem_type == \"classification\":\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MAIN LOOP\n",
    "# =========================================================\n",
    "for dataset in datasets:\n",
    "\n",
    "    print(f\"\\n================ DATASET: {dataset} ================\\n\")\n",
    "\n",
    "    for desc in descriptor_types:\n",
    "\n",
    "        print(f\"--- Descriptor: {desc} ---\")\n",
    "\n",
    "        file_path = os.path.join(descriptor_folder, f\"{dataset}_{desc}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"❌ File missing\")\n",
    "            continue\n",
    "\n",
    "        # ---------- Save directory ----------\n",
    "        save_dir = os.path.join(results_folder, dataset, desc)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # ---------- Load ----------\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "        # ---------- Target detection ----------\n",
    "        target_cols = [c for c in df.columns if c.lower() in [\"y\", \"label\", \"target\", \"value\"]]\n",
    "        if not target_cols:\n",
    "            print(\"❌ Target column not found\")\n",
    "            continue\n",
    "\n",
    "        TARGET_COL = target_cols[0]\n",
    "        Y = df[TARGET_COL].values\n",
    "\n",
    "        drop_cols = [TARGET_COL]\n",
    "        if SMILES_COL in df.columns:\n",
    "            drop_cols.append(SMILES_COL)\n",
    "\n",
    "        X = df.drop(columns=drop_cols)\n",
    "\n",
    "        # ---------- Descriptor cleanup ----------\n",
    "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        X.fillna(0, inplace=True)\n",
    "        X = X.loc[:, X.var() > 0]\n",
    "\n",
    "        if X.shape[1] < 10:\n",
    "            print(\"❌ Too few descriptors\")\n",
    "            continue\n",
    "\n",
    "        # ---------- Problem type ----------\n",
    "        problem_type = (\n",
    "            \"classification\"\n",
    "            if set(np.unique(Y)).issubset({0, 1})\n",
    "            else \"regression\"\n",
    "        )\n",
    "\n",
    "        # ---------- Split ----------\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X.values, Y,\n",
    "            test_size=0.1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        scaler = MinMaxScaler() if problem_type == \"classification\" else StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # ---------- Reshape for sequence models ----------\n",
    "        X_train_seq = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "        X_test_seq = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "        ]\n",
    "\n",
    "        # =================================================\n",
    "        # CNN\n",
    "        # =================================================\n",
    "        print(\"🚀 Training CNN\")\n",
    "        cnn = build_cnn(X_train_seq.shape[1:], problem_type)\n",
    "        cnn.fit(\n",
    "            X_train_seq, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = cnn.predict(X_test_seq).ravel()\n",
    "        if problem_type == \"classification\":\n",
    "            acc = accuracy_score(y_test, (y_pred >= 0.5).astype(int))\n",
    "            print(f\"✅ CNN Accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"✅ CNN RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "        cnn.save(os.path.join(save_dir, \"CNN_model\"))\n",
    "        print(\"💾 CNN model saved\")\n",
    "\n",
    "        # =================================================\n",
    "        # RNN\n",
    "        # =================================================\n",
    "        print(\"🚀 Training RNN\")\n",
    "        rnn = build_rnn(X_train_seq.shape[1:], problem_type)\n",
    "        rnn.fit(\n",
    "            X_train_seq, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = rnn.predict(X_test_seq).ravel()\n",
    "        if problem_type == \"classification\":\n",
    "            acc = accuracy_score(y_test, (y_pred >= 0.5).astype(int))\n",
    "            print(f\"✅ RNN Accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"✅ RNN RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "        rnn.save(os.path.join(save_dir, \"RNN_model\"))\n",
    "        print(\"💾 RNN model saved\")\n",
    "\n",
    "        # =================================================\n",
    "        # LSTM\n",
    "        # =================================================\n",
    "        print(\"🚀 Training LSTM\")\n",
    "        lstm = build_lstm(X_train_seq.shape[1:], problem_type)\n",
    "        lstm.fit(\n",
    "            X_train_seq, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        y_pred = lstm.predict(X_test_seq).ravel()\n",
    "        if problem_type == \"classification\":\n",
    "            acc = accuracy_score(y_test, (y_pred >= 0.5).astype(int))\n",
    "            print(f\"✅ LSTM Accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"✅ LSTM RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "        lstm.save(os.path.join(save_dir, \"LSTM_model\"))\n",
    "        print(\"💾 LSTM model saved\")\n",
    "\n",
    "        print(\"✅ Finished:\", dataset, desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c4616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after fuckups in linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d2bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 09:38:50,081 | INFO | ===== ADMET TRAINING STARTED =====\n",
      "2026-01-31 09:39:04,972 | INFO | GPU devices: CPU only\n",
      "2026-01-31 09:39:04,972 | INFO | TensorFlow version: 2.20.0\n",
      "2026-01-31 09:39:05,443 | INFO | Results directory: E:\\neuro lab\\sem 8\\pk-vibhuvan\\results\n",
      "2026-01-31 09:39:05,444 | INFO | ===== DATASET: vdss_lombardo =====\n",
      "2026-01-31 09:39:06,035 | INFO | vdss_lombardo | Descriptors: ('mordred',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vibhu\\AppData\\Local\\Temp\\ipykernel_27432\\1847953211.py:188: DtypeWarning: Columns (138,139,140,147,148,149,156,157,158,165,166,167,174,175,176,183,184,185,192,193,194,201,202,203,210,211,212,219,220,221,228,229,230,345,346,347,354,355,356,363,364,365,372,373,374,381,382,383,390,391,392,399,400,401,408,409,410,417,418,419,426,427,428,435,436,437,444,445,446,452,453,454,460,461,462,468,469,470,476,477,478,484,485,486,492,493,494,500,501,502,508,509,510,516,517,518,524,525,526,532,533,534,540,541,542,548,549,550,556,557,558,564,565,566,572,573,574,580,581,582,588,589,590,596,597,598,604,605,606,612,613,614,620,621,622,628,629,630,636,637,638,791,828,829,830,831,844,845,846,847,1301) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  desc_dfs[d] = pd.read_csv(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-31 09:39:06,317 | INFO | Shapes: X_train (904, 1000, 1), X_test (226, 1000, 1)\n",
      "2026-01-31 09:39:06,317 | INFO | Skipping existing: vdss_lombardo__mordred__cnn__regression\n",
      "2026-01-31 09:39:06,317 | INFO | Training: vdss_lombardo__mordred__rnn__regression\n",
      "2026-01-31 09:39:07,414 | WARNING | From c:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 251\u001b[39m\n\u001b[32m    247\u001b[39m tf.keras.backend.clear_session()\n\u001b[32m    249\u001b[39m model = ModelFactory.build(model_type, X_tr.shape[\u001b[32m1\u001b[39m:], problem_type)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m preds = model.predict(X_te, batch_size=BATCH_SIZE, verbose=\u001b[32m0\u001b[39m).flatten()\n\u001b[32m    262\u001b[39m metrics = (\n\u001b[32m    263\u001b[39m     regression_metrics(y_te, preds)\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m problem_type == \u001b[33m\"\u001b[39m\u001b[33mregression\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m classification_metrics(y_te, preds)\n\u001b[32m    266\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:810\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_functions_eagerly:\n\u001b[32m    809\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[38;5;28mself\u001b[39m._name, tf_function_call=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:133\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m         )\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    137\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:810\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_functions_eagerly:\n\u001b[32m    809\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m trace.Trace(\u001b[38;5;28mself\u001b[39m._name, tf_function_call=\u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[32m    814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:114\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    116\u001b[39m         outputs,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    118\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     )\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:58\u001b[39m, in \u001b[36mTensorFlowTrainer.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_has_training_arg:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     60\u001b[39m         y_pred = \u001b[38;5;28mself\u001b[39m(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\sequential.py:220\u001b[39m, in \u001b[36mSequential.call\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._functional:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_functional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m     \u001b[38;5;66;03m# Fallback: Just apply the layer sequence.\u001b[39;00m\n\u001b[32m    225\u001b[39m     \u001b[38;5;66;03m# This typically happens if `inputs` is a nested struct.\u001b[39;00m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    227\u001b[39m         \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and\u001b[39;00m\n\u001b[32m    228\u001b[39m         \u001b[38;5;66;03m# `outputs` are the outputs of `layer` applied to `inputs`. At the\u001b[39;00m\n\u001b[32m    229\u001b[39m         \u001b[38;5;66;03m# end of each iteration `inputs` is set to `outputs` to prepare for\u001b[39;00m\n\u001b[32m    230\u001b[39m         \u001b[38;5;66;03m# the next layer.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:183\u001b[39m, in \u001b[36mFunctional.call\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m             backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\function.py:206\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    204\u001b[39m     operation = \u001b[38;5;28mself\u001b[39m._get_operation_for_node(node)\n\u001b[32m    205\u001b[39m     op = operation_fn(operation)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     outputs = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node.outputs, tree.flatten(outputs)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\models\\functional.py:644\u001b[39m, in \u001b[36moperation_fn.<locals>.call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    639\u001b[39m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[33m\"\u001b[39m\u001b[33m_call_context_args\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    640\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    641\u001b[39m     ):\n\u001b[32m    642\u001b[39m         kwargs[name] = value\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\simple_rnn.py:353\u001b[39m, in \u001b[36mSimpleRNN.call\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences, initial_state=\u001b[38;5;28;01mNone\u001b[39;00m, mask=\u001b[38;5;28;01mNone\u001b[39;00m, training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_state\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:406\u001b[39m, in \u001b[36mRNN.call\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# Prepopulate the dropout state so that the inner_loop is stateless\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# this is particularly important for JAX backend.\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_config_dropout_masks(\n\u001b[32m    403\u001b[39m     \u001b[38;5;28mself\u001b[39m.cell, sequences[:, \u001b[32m0\u001b[39m, :], initial_state\n\u001b[32m    404\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m last_output, outputs, states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m last_output = ops.cast(last_output, \u001b[38;5;28mself\u001b[39m.compute_dtype)\n\u001b[32m    413\u001b[39m outputs = ops.cast(outputs, \u001b[38;5;28mself\u001b[39m.compute_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:346\u001b[39m, in \u001b[36mRNN.inner_loop\u001b[39m\u001b[34m(self, sequences, initial_state, mask, training)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tree.is_nested(initial_state):\n\u001b[32m    344\u001b[39m     initial_state = [initial_state]\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43munroll\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_all_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py:218\u001b[39m, in \u001b[36mrnn\u001b[39m\u001b[34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(time_steps):\n\u001b[32m    217\u001b[39m     inp = _get_input_tensor(i)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     output, states = \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_all_outputs:\n\u001b[32m    222\u001b[39m         successive_outputs.append(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:338\u001b[39m, in \u001b[36mRNN.inner_loop.<locals>.step\u001b[39m\u001b[34m(inputs, states)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.backend() == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stateful:\n\u001b[32m    337\u001b[39m     states = tree.map_structure(ops.copy, states)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m output, new_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcell_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tree.is_nested(new_states):\n\u001b[32m    340\u001b[39m     new_states = [new_states]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:941\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    939\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    943\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    945\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\simple_rnn.py:161\u001b[39m, in \u001b[36mSimpleRNNCell.call\u001b[39m\u001b[34m(self, sequence, states, training)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;129;01mand\u001b[39;00m dp_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     sequence = sequence * dp_mask\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m h = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m     h += \u001b[38;5;28mself\u001b[39m.bias\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:4211\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   4209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   4210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Matmul().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m4211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:520\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    518\u001b[39m     output_type = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    519\u001b[39m x1 = tf.cast(x1, compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m x2 = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_combined_batch_dimensions\u001b[39m(a, b, output_shape, fn_3d):\n\u001b[32m    523\u001b[39m     a_sparse = \u001b[38;5;28misinstance\u001b[39m(a, tf.SparseTensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1266\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1012\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(x, dtype, name)\u001b[39m\n\u001b[32m   1006\u001b[39m   x = indexed_slices.IndexedSlices(values_cast, x.indices, x.dense_shape)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1008\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[32m   1009\u001b[39m   \u001b[38;5;66;03m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[32m   1010\u001b[39m   \u001b[38;5;66;03m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[32m   1011\u001b[39m   \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m   x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m x.dtype.is_complex \u001b[38;5;129;01mand\u001b[39;00m base_type.is_floating:\n\u001b[32m   1014\u001b[39m     logging.warn(\n\u001b[32m   1015\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are casting an input of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1016\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.  This will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscard the imaginary part and may not be what you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mintended.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[39m, in \u001b[36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:757\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    756\u001b[39m preferred_dtype = preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:209\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    207\u001b[39m overload = \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33m__tf_tensor__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[32m    212\u001b[39m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[32m    214\u001b[39m   ret = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:84\u001b[39m, in \u001b[36mVariable.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1266\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001b[39m, in \u001b[36mconvert_to_tensor_v2_with_dispatch\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m.tf_export(\u001b[33m\"\u001b[39m\u001b[33mconvert_to_tensor\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m     97\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[32m     99\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m ) -> tensor_lib.Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001b[39m, in \u001b[36mconvert_to_tensor_v2\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2378\u001b[39m, in \u001b[36m_dense_var_to_tensor\u001b[39m\u001b[34m(var, dtype, name, as_ref)\u001b[39m\n\u001b[32m   2377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dense_var_to_tensor\u001b[39m(var, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m, as_ref=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1624\u001b[39m, in \u001b[36mBaseResourceVariable._dense_var_to_tensor\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1622\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_value().op.inputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:658\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:843\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    841\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    846\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    847\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    848\u001b[39m   record.record_operation(\n\u001b[32m    849\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    850\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    851\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat.forward_compatible(\u001b[32m2022\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m):\n\u001b[32m    832\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m result = \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m._dtype, \u001b[38;5;28mself\u001b[39m.handle, result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vibhu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:534\u001b[39m, in \u001b[36mread_variable_op\u001b[39m\u001b[34m(resource, dtype, name)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    533\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadVariableOp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    537\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ULTRA-SAFE ADMET DL PIPELINE (LOGGING + TRY/EXCEPT)\n",
    "# =========================================================\n",
    "\n",
    "# ---------------- HARD XLA DISABLE ----------------\n",
    "import os\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "os.environ[\"TF_ENABLE_XLA\"] = \"0\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"\"\n",
    "\n",
    "# ---------------- CORE ----------------\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- LOGGING SETUP ----------------\n",
    "LOG_PATH = \"training.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "log.info(\"===== ADMET TRAINING STARTED =====\")\n",
    "\n",
    "# ---------------- TENSORFLOW ----------------\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "log.info(f\"GPU devices: {gpus if gpus else 'CPU only'}\")\n",
    "log.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# ---------------- SKLEARN ----------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    roc_auc_score, accuracy_score, f1_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---------------- KERAS ----------------\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv1D, Flatten,\n",
    "    SimpleRNN, LSTM,\n",
    "    InputLayer, Dropout\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# =========================================================\n",
    "# PATHS\n",
    "# =========================================================\n",
    "base_win_path = r\"E:\\neuro lab\\sem 8\\pk-vibhuvan\"\n",
    "descriptor_folder = os.path.join(base_win_path, \"descriptors\")\n",
    "cleaned_folder = os.path.join(base_win_path, \"descriptors_cleaned\")\n",
    "results_folder = os.path.join(base_win_path, \"results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "if os.name == \"posix\":\n",
    "    descriptor_folder = descriptor_folder.replace(\"E:\\\\\", \"/mnt/e/\").replace(\"\\\\\", \"/\")\n",
    "    cleaned_folder = cleaned_folder.replace(\"E:\\\\\", \"/mnt/e/\").replace(\"\\\\\", \"/\")\n",
    "    results_folder = results_folder.replace(\"E:\\\\\", \"/mnt/e/\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "log.info(f\"Results directory: {results_folder}\")\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "datasets = [ \"vdss_lombardo\",\"ppbr_az\"]\n",
    "descriptor_types = [\"mordred\", \"rdkit\", \"maccs\"]\n",
    "model_types = [\"cnn\", \"rnn\", \"lstm\"]\n",
    "\n",
    "MAX_EPOCHS = 25\n",
    "BATCH_SIZE = 2\n",
    "MAX_FEATURES = 1000\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "# =========================================================\n",
    "# HARD SKIP LIST (EXPLICIT RUN LABELS)\n",
    "# =========================================================\n",
    "HARD_SKIP = {\n",
    "    \"ppbr_az__mordred__cnn__regression\",\n",
    "    \"ppbr_az__mordred__rnn__regression\",\n",
    "    \"ppbr_az__mordred__lstm__regression\",\n",
    "    \"ppbr_az__rdkit__cnn__regression\",\n",
    "}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# MODEL FACTORY (ULTRA-MINIMAL)\n",
    "# =========================================================\n",
    "class ModelFactory:\n",
    "    @staticmethod\n",
    "    def build(model_type, input_shape, problem_type):\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(shape=input_shape))\n",
    "\n",
    "        if model_type == \"cnn\":\n",
    "            model.add(Conv1D(4, 3, padding=\"same\", activation=\"relu\"))\n",
    "            model.add(Flatten())\n",
    "\n",
    "        elif model_type == \"rnn\":\n",
    "            model.add(SimpleRNN(8, unroll=True))\n",
    "\n",
    "        elif model_type == \"lstm\":\n",
    "            model.add(LSTM(8, unroll=True))\n",
    "\n",
    "        model.add(Dense(8, activation=\"relu\"))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        if problem_type == \"regression\":\n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "        else:\n",
    "            model.add(Dense(1, activation=\"sigmoid\"))\n",
    "            model.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=[\"accuracy\"],\n",
    "            )\n",
    "\n",
    "        return model\n",
    "\n",
    "# =========================================================\n",
    "# METRICS\n",
    "# =========================================================\n",
    "def regression_metrics(y, p):\n",
    "    return {\n",
    "        \"R2\": r2_score(y, p),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y, p)),\n",
    "        \"MAE\": mean_absolute_error(y, p),\n",
    "        \"Spearman\": spearmanr(y, p).correlation,\n",
    "    }\n",
    "\n",
    "def classification_metrics(y, p):\n",
    "    yb = (p > 0.5).astype(int)\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, yb),\n",
    "        \"F1\": f1_score(y, yb),\n",
    "        \"ROC_AUC\": roc_auc_score(y, p),\n",
    "        \"PR_AUC\": auc(rec, prec),\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# TRAIN LOOP (FULLY GUARDED)\n",
    "# =========================================================\n",
    "summary = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    log.info(f\"===== DATASET: {dataset} =====\")\n",
    "\n",
    "    try:\n",
    "        y_df = pd.read_csv(os.path.join(cleaned_folder, f\"{dataset}.csv\"))\n",
    "        y = y_df[\"Y\"].values\n",
    "        problem_type = \"classification\" if set(np.unique(y)) <= {0, 1} else \"regression\"\n",
    "    except Exception:\n",
    "        log.error(f\"Failed to load target for {dataset}\")\n",
    "        log.error(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    desc_dfs = {}\n",
    "    for d in descriptor_types:\n",
    "        try:\n",
    "            path = os.path.join(descriptor_folder, f\"{dataset}_{d}.csv\")\n",
    "            if os.path.exists(path):\n",
    "                desc_dfs[d] = pd.read_csv(path)\n",
    "        except Exception:\n",
    "            log.warning(f\"Failed loading descriptor {d} for {dataset}\")\n",
    "\n",
    "    for r in range(1, len(descriptor_types) + 1):\n",
    "        for comb in combinations(descriptor_types, r):\n",
    "            if not all(c in desc_dfs for c in comb):\n",
    "                continue\n",
    "\n",
    "            log.info(f\"{dataset} | Descriptors: {comb}\")\n",
    "\n",
    "            try:\n",
    "                X = pd.concat(\n",
    "                    [desc_dfs[c].drop(columns=[\"smiles\"], errors=\"ignore\") for c in comb],\n",
    "                    axis=1,\n",
    "                ).apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "                if X.shape[1] > MAX_FEATURES:\n",
    "                    X = X.iloc[:, :MAX_FEATURES]\n",
    "\n",
    "                X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "                    X.values, y, test_size=0.2, random_state=42\n",
    "                )\n",
    "\n",
    "                scaler = StandardScaler() if problem_type == \"regression\" else MinMaxScaler()\n",
    "                X_tr = scaler.fit_transform(X_tr)\n",
    "                X_te = scaler.transform(X_te)\n",
    "\n",
    "                X_tr = X_tr[..., None]\n",
    "                X_te = X_te[..., None]\n",
    "\n",
    "                log.info(f\"Shapes: X_train {X_tr.shape}, X_test {X_te.shape}\")\n",
    "\n",
    "            except Exception:\n",
    "                log.error(\"Preprocessing failed\")\n",
    "                log.error(traceback.format_exc())\n",
    "                continue\n",
    "\n",
    "            for model_type in model_types:\n",
    "                label = f\"{dataset}__{'+'.join(comb)}__{model_type}__{problem_type}\"\n",
    "\n",
    "                # ---------- HARD SKIP ----------\n",
    "                if label in HARD_SKIP:\n",
    "                    log.info(f\" HARD-SKIPPED (user-defined): {label}\")\n",
    "                    continue\n",
    "                # --------------------------------\n",
    "\n",
    "                save_dir = os.path.join(results_folder, dataset, \"+\".join(comb), model_type)\n",
    "                model_path = os.path.join(save_dir, f\"{label}.keras\")\n",
    "\n",
    "\n",
    "                if os.path.exists(model_path):\n",
    "                    log.info(f\"Skipping existing: {label}\")\n",
    "                    continue\n",
    "\n",
    "                log.info(f\"Training: {label}\")\n",
    "\n",
    "                try:\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                    model = ModelFactory.build(model_type, X_tr.shape[1:], problem_type)\n",
    "\n",
    "                    model.fit(\n",
    "                        X_tr, y_tr,\n",
    "                        validation_split=0.1,\n",
    "                        epochs=MAX_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=0,\n",
    "                    )\n",
    "\n",
    "                    preds = model.predict(X_te, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "\n",
    "                    metrics = (\n",
    "                        regression_metrics(y_te, preds)\n",
    "                        if problem_type == \"regression\"\n",
    "                        else classification_metrics(y_te, preds)\n",
    "                    )\n",
    "\n",
    "                    pd.DataFrame([metrics]).to_csv(\n",
    "                        os.path.join(save_dir, \"metrics.csv\"), index=False\n",
    "                    )\n",
    "\n",
    "                    model.save(model_path)\n",
    "\n",
    "                    model_cpu = tf.keras.models.clone_model(model)\n",
    "                    model_cpu.set_weights(model.get_weights())\n",
    "                    with open(os.path.join(save_dir, f\"{label}.pkl\"), \"wb\") as f:\n",
    "                        pickle.dump(model_cpu, f)\n",
    "\n",
    "                    summary.append({\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Descriptors\": \"+\".join(comb),\n",
    "                        \"Model\": model_type,\n",
    "                        \"Problem\": problem_type,\n",
    "                        **metrics,\n",
    "                    })\n",
    "\n",
    "                    log.info(f\"✓ Completed: {label}\")\n",
    "\n",
    "                except Exception:\n",
    "                    log.error(f\"FAILED: {label}\")\n",
    "                    log.error(traceback.format_exc())\n",
    "\n",
    "                finally:\n",
    "                    del model\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    gc.collect()\n",
    "\n",
    "# =========================================================\n",
    "# SAVE SUMMARY\n",
    "# =========================================================\n",
    "pd.DataFrame(summary).to_csv(\n",
    "    os.path.join(results_folder, \"summary_results.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "log.info(\"===== ALL TRAINING COMPLETED SAFELY =====\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
